{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-03T03:09:08.043764700Z",
     "start_time": "2024-12-03T03:09:08.010379Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import optimize\n",
    "from scipy import special\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve, accuracy_score, roc_auc_score, log_loss\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "\n",
    "from src.data_prep import prepare_data\n",
    "# from src.models import logistic_regression_model, decision_tree_model, random_forest_model, lightgbm_model\n",
    "# from src.train import train_sklearn_model\n",
    "from src.evaluation import evaluate_model\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import joblib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# df = pd.read_csv('data/creditcard/creditcard.csv')\n",
    "# X = df.drop(columns='Class')\n",
    "# y = df['Class']\n",
    "\n",
    "df = pd.read_csv('data/loan/loandefault.csv')\n",
    "df = df.drop([\"LoanID\"],axis = 1)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "for i in df.columns[9:16]:\n",
    "    df[i] = encoder.fit_transform(df[i])\n",
    "    \n",
    "x,y = df.iloc[:,:16], df.iloc[:,16]\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(x)\n",
    "#######################\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "X_fit, X_val, y_fit, y_val = train_test_split(\n",
    "    X_train, y_train,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# X_train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-03T03:09:11.661690900Z",
     "start_time": "2024-12-03T03:09:10.174512400Z"
    }
   },
   "id": "304a2f19fb78401d",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 16695, number of negative: 126937\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002868 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1303\n",
      "[LightGBM] [Info] Number of data points in the train set: 143632, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116235 -> initscore=-2.028582\n",
      "[LightGBM] [Info] Start training from score -2.028582\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tfit's binary_logloss: 0.325066\tval's binary_logloss: 0.325833\n",
      "[200]\tfit's binary_logloss: 0.315383\tval's binary_logloss: 0.318095\n",
      "[300]\tfit's binary_logloss: 0.310531\tval's binary_logloss: 0.315022\n",
      "[400]\tfit's binary_logloss: 0.30747\tval's binary_logloss: 0.313654\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[400]\tfit's binary_logloss: 0.30747\tval's binary_logloss: 0.313654\n",
      "\n",
      "Test's ROC AUC: 0.75166\n",
      "Test's logloss: 0.31451\n"
     ]
    }
   ],
   "source": [
    "fit = lgb.Dataset(X_fit, y_fit)\n",
    "val = lgb.Dataset(X_val, y_val, reference=fit)\n",
    "\n",
    "model = lgb.train(\n",
    "    params={\n",
    "        'learning_rate': 0.01,\n",
    "        'objective': 'binary'\n",
    "    },\n",
    "    train_set=fit,\n",
    "    num_boost_round=400,\n",
    "    valid_sets=(fit, val),\n",
    "    valid_names=('fit', 'val'),\n",
    "  callbacks=[\n",
    "    lgb.early_stopping(stopping_rounds=20),\n",
    "    lgb.log_evaluation(period=100)\n",
    "    ]\n",
    ")\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print()\n",
    "print(f\"Test's ROC AUC: {roc_auc_score(y_test, y_pred):.5f}\")\n",
    "print(f\"Test's logloss: {log_loss(y_test, y_pred):.5f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-03T03:09:19.873057200Z",
     "start_time": "2024-12-03T03:09:16.476418Z"
    }
   },
   "id": "7ab3919d456bbeea",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary predictions saved successfully!\n"
     ]
    }
   ],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "\n",
    "optimal_idx = (tpr - fpr).argmax()\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "y_pred_binary = (y_pred >= optimal_threshold).astype(int)\n",
    "# y_pred_binary = (y_pred >= 0.5).astype(int)\n",
    "\n",
    "# Save binary predictions and true labels\n",
    "# binary_predictions = pd.DataFrame({\"y_true\": y_test, \"y_pred\": y_pred_binary})\n",
    "# binary_predictions.to_csv(\"artifacts/predictions/lightgbm_bce_predictions.csv\", index=False)\n",
    "\n",
    "predictions = pd.DataFrame({\"y_true\": y_test, \"y_pred\": y_pred})\n",
    "predictions.to_csv(\"artifacts/lightgbm_preds/loan/lightgbm_bce_continuous_predictions.csv\", index=False)\n",
    "\n",
    "# print(f\"Optimal Threshold: {optimal_threshold:.5f}\")\n",
    "print(\"Binary predictions saved successfully!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-28T21:36:41.606343800Z",
     "start_time": "2024-11-28T21:36:41.469720600Z"
    }
   },
   "id": "475ac7123eb26889",
   "execution_count": 157
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class FocalLoss:\n",
    "\n",
    "    def __init__(self, gamma, alpha=None):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def at(self, y):\n",
    "        if self.alpha is None:\n",
    "            return np.ones_like(y)\n",
    "        return np.where(y, self.alpha, 1 - self.alpha)\n",
    "\n",
    "    def pt(self, y, p):\n",
    "        p = np.clip(p, 1e-15, 1 - 1e-15)\n",
    "        return np.where(y, p, 1 - p)\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        at = self.at(y_true)\n",
    "        pt = self.pt(y_true, y_pred)\n",
    "        return -at * (1 - pt) ** self.gamma * np.log(pt)\n",
    "\n",
    "    def grad(self, y_true, y_pred):\n",
    "        y = 2 * y_true - 1  # {0, 1} -> {-1, 1}\n",
    "        at = self.at(y_true)\n",
    "        pt = self.pt(y_true, y_pred)\n",
    "        g = self.gamma\n",
    "        return at * y * (1 - pt) ** g * (g * pt * np.log(pt) + pt - 1)\n",
    "\n",
    "    def hess(self, y_true, y_pred):\n",
    "        y = 2 * y_true - 1  # {0, 1} -> {-1, 1}\n",
    "        at = self.at(y_true)\n",
    "        pt = self.pt(y_true, y_pred)\n",
    "        g = self.gamma\n",
    "\n",
    "        u = at * y * (1 - pt) ** g\n",
    "        du = -at * y * g * (1 - pt) ** (g - 1)\n",
    "        v = g * pt * np.log(pt) + pt - 1\n",
    "        dv = g * np.log(pt) + g + 1\n",
    "\n",
    "        return (du * v + u * dv) * y * (pt * (1 - pt))\n",
    "\n",
    "    def init_score(self, y_true):\n",
    "        res = optimize.minimize_scalar(\n",
    "            lambda p: self(y_true, p).sum(),\n",
    "            bounds=(0, 1),\n",
    "            method='bounded'\n",
    "        )\n",
    "        p = res.x\n",
    "        log_odds = np.log(p / (1 - p))\n",
    "        return log_odds\n",
    "\n",
    "    def lgb_obj(self, preds, train_data):\n",
    "        y = train_data.get_label()\n",
    "        p = special.expit(preds)\n",
    "        return self.grad(y, p), self.hess(y, p)\n",
    "\n",
    "    def lgb_eval(self, preds, train_data):\n",
    "        y = train_data.get_label()\n",
    "        p = special.expit(preds)\n",
    "        is_higher_better = False\n",
    "        return 'focal_loss', self(y, p).mean(), is_higher_better"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-28T21:52:31.696465700Z",
     "start_time": "2024-11-28T21:52:31.672833300Z"
    }
   },
   "id": "39d80f1b41e49891",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002856 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1303\n",
      "[LightGBM] [Info] Number of data points in the train set: 143632, number of used features: 16\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tfit's focal_loss: 0.325066\tval's focal_loss: 0.325833\n",
      "[200]\tfit's focal_loss: 0.315383\tval's focal_loss: 0.318095\n",
      "[300]\tfit's focal_loss: 0.310531\tval's focal_loss: 0.315022\n",
      "[400]\tfit's focal_loss: 0.30747\tval's focal_loss: 0.313654\n",
      "[500]\tfit's focal_loss: 0.305233\tval's focal_loss: 0.312942\n",
      "[600]\tfit's focal_loss: 0.303328\tval's focal_loss: 0.312637\n",
      "[700]\tfit's focal_loss: 0.301592\tval's focal_loss: 0.312523\n",
      "[800]\tfit's focal_loss: 0.299999\tval's focal_loss: 0.312454\n",
      "Early stopping, best iteration is:\n",
      "[811]\tfit's focal_loss: 0.299824\tval's focal_loss: 0.312444\n",
      "\n",
      "Test's ROC AUC: 0.75384\n",
      "Test's logloss: 0.31303\n"
     ]
    }
   ],
   "source": [
    "fl = FocalLoss(alpha=None, gamma=0)\n",
    "\n",
    "fit = lgb.Dataset(\n",
    "    X_fit, y_fit,\n",
    "    init_score=np.full_like(y_fit, fl.init_score(y_fit), dtype=float)\n",
    ")\n",
    "\n",
    "val = lgb.Dataset(\n",
    "    X_val, y_val,\n",
    "    init_score=np.full_like(y_val, fl.init_score(y_fit), dtype=float),\n",
    "    reference=fit\n",
    ")\n",
    "\n",
    "model = lgb.train(\n",
    "    params={\n",
    "        'learning_rate': 0.01,\n",
    "        'objective': fl.lgb_obj\n",
    "    },\n",
    "    train_set=fit,\n",
    "    num_boost_round=10000,\n",
    "    valid_sets=(fit, val),\n",
    "    valid_names=('fit', 'val'),\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=20),\n",
    "        lgb.log_evaluation(period=100)\n",
    "    ],\n",
    "    feval=fl.lgb_eval\n",
    ")\n",
    "\n",
    "y_pred = special.expit(fl.init_score(y_fit) + model.predict(X_test))\n",
    "\n",
    "print()\n",
    "print(f\"Test's ROC AUC: {roc_auc_score(y_test, y_pred):.5f}\")\n",
    "print(f\"Test's logloss: {log_loss(y_test, y_pred):.5f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-28T21:37:58.884162700Z",
     "start_time": "2024-11-28T21:36:41.638357600Z"
    }
   },
   "id": "40026623f755db8d",
   "execution_count": 159
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Threshold: 0.11746\n",
      "Binary predictions saved successfully!\n"
     ]
    }
   ],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "\n",
    "optimal_idx = (tpr - fpr).argmax()\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "y_pred_binary = (y_pred >= optimal_threshold).astype(int)\n",
    "\n",
    "# y_pred_binary = (y_pred >= 0.5).astype(int)\n",
    "\n",
    "# binary_predictions = pd.DataFrame({\"y_true\": y_test, \"y_pred\": y_pred_binary})\n",
    "# binary_predictions.to_csv(\"artifacts/predictions/lightgbm_focal_loss_predictions.csv\", index=False)\n",
    "\n",
    "predictions = pd.DataFrame({\"y_true\": y_test, \"y_pred\": y_pred})\n",
    "predictions.to_csv(\"artifacts/lightgbm_preds/loan/lightgbm_focal_loss_continuous_predictions.csv\", index=False)\n",
    "\n",
    "print(f\"Optimal Threshold: {optimal_threshold:.5f}\")\n",
    "print(\"Binary predictions saved successfully!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-28T21:37:59.021602500Z",
     "start_time": "2024-11-28T21:37:58.888554800Z"
    }
   },
   "id": "f4c42b6865dfb794",
   "execution_count": 160
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ROC-Star"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6b7a47f3d3ca3d7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ROCStarLoss:\n",
    "\n",
    "    def __init__(self, delta=2, gamma=0.4):\n",
    "\n",
    "        self.delta = delta\n",
    "\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.epoch_true = None\n",
    "\n",
    "        self.epoch_pred = None\n",
    "\n",
    "        self.bce_epoch = 0\n",
    "\n",
    "        self.BCE = nn.BCELoss()\n",
    "\n",
    " \n",
    "\n",
    "    def calc_loss(self, y_true, y_pred):\n",
    "\n",
    "       \n",
    "\n",
    "        # If first epoch, BCE Loss\n",
    "\n",
    "        if self.bce_epoch > 0:\n",
    "\n",
    "            self.bce_epoch -= 1\n",
    "\n",
    "            self.epoch_true = y_true.clone()\n",
    "\n",
    "            self.epoch_pred = y_pred.clone()\n",
    "\n",
    " \n",
    "\n",
    "            return self.BCE(y_pred, y_true)\n",
    "\n",
    " \n",
    "\n",
    "        # B/W cmparison for Appeal/No Appeal - [B(+), W(-)]\n",
    "\n",
    " \n",
    "\n",
    "        pos_ind = y_true >= 0.5\n",
    "\n",
    "        neg_ind = y_true < 0.5\n",
    "\n",
    " \n",
    "\n",
    "        B = y_pred[pos_ind]\n",
    "\n",
    "        W = y_pred[neg_ind]\n",
    "\n",
    " \n",
    "\n",
    "        B_shifted = B - self.gamma\n",
    "\n",
    " \n",
    "\n",
    "        # Batch-wise loss calculatoin\n",
    "\n",
    "        batch_size = 30000\n",
    "\n",
    "        loss = 0.0\n",
    "\n",
    " \n",
    "\n",
    "        for i in range(0, len(W), batch_size):\n",
    "\n",
    "            W_batch = W[i:i + batch_size]\n",
    "\n",
    " \n",
    "\n",
    "            comparisons_batch = W_batch.unsqueeze(1) - B_shifted.unsqueeze(0)\n",
    "\n",
    " \n",
    "\n",
    "            loss += torch.sum((torch.clamp(comparisons_batch, min=0))**2)\n",
    "\n",
    " \n",
    "\n",
    "        # end code #\n",
    "\n",
    " \n",
    "\n",
    "        # full dataset loss\n",
    "\n",
    "        # comparisons = W.unsqueeze(1) - B_shifted.unsqueeze(0)\n",
    "\n",
    " \n",
    "\n",
    "        # loss_matrix = torch.clamp(comparisons, min=0)\n",
    "\n",
    "        # loss2 = torch.sum(loss_matrix ** 2)\n",
    "\n",
    "        # print(\"Losses for full and batch-wise\")\n",
    "\n",
    "        # print(loss)\n",
    "\n",
    "        # print(loss2)\n",
    "\n",
    "        # end code #\n",
    "\n",
    " \n",
    "\n",
    "        return loss\n",
    "\n",
    " \n",
    "\n",
    "    def init_score(self, y_true):\n",
    "\n",
    "        p = np.mean(y_true)\n",
    "\n",
    "        p = np.clip(p, 1e-15, 1 - 1e-15)\n",
    "\n",
    "        log_odds = np.log(p / (1 - p))\n",
    "\n",
    "        return log_odds\n",
    "\n",
    " \n",
    "\n",
    "    def grad(self, y_true, y_pred):\n",
    "\n",
    "        B_ind = y_true >= 0.5\n",
    "\n",
    "        W_ind = y_true < 0.5\n",
    "\n",
    " \n",
    "\n",
    "        B = y_pred[B_ind]\n",
    "\n",
    "        W = y_pred[W_ind]\n",
    "\n",
    " \n",
    "\n",
    "        dB = B * (1 - B)\n",
    "\n",
    "        dW = W * (1 - W)\n",
    "\n",
    " \n",
    "\n",
    "        # Batching for GPU Memory saving\n",
    "\n",
    "        batch_size = 30000\n",
    "\n",
    " \n",
    "\n",
    "        sum_comparisons_B = torch.zeros_like(B)\n",
    "\n",
    "        sum_comparisons_W = torch.zeros_like(W)\n",
    "\n",
    " \n",
    "\n",
    "        for i in range(0, len(W), batch_size):\n",
    "\n",
    "            W_batch = W[i: i + batch_size]\n",
    "\n",
    " \n",
    "\n",
    "            comparisons_B_batch = W_batch.unsqueeze(1) - B.unsqueeze(0) + self.gamma\n",
    "\n",
    " \n",
    "\n",
    "            sum_comparisons_B_batch = torch.clamp(comparisons_B_batch, min=0).sum(dim=0)\n",
    "\n",
    "            sum_comparisons_W_batch = torch.clamp(comparisons_B_batch, min=0).sum(dim=1)\n",
    "\n",
    " \n",
    "\n",
    "            sum_comparisons_B += sum_comparisons_B_batch\n",
    "\n",
    "            sum_comparisons_W[i:i + batch_size] += sum_comparisons_W_batch\n",
    "\n",
    " \n",
    "\n",
    "        dLdx = -2 * sum_comparisons_B * dB\n",
    "\n",
    "        dLdy = 2 * sum_comparisons_W * dW\n",
    "\n",
    " \n",
    "\n",
    "        # end code #\n",
    "\n",
    " \n",
    "\n",
    "        # Normal matrix code for full dataset\n",
    "\n",
    "        # comparisons = W.unsqueeze(1) - B.unsqueeze(0) + self.gamma\n",
    "\n",
    "        # comparisons = torch.clamp(comparisons, min=0)\n",
    "\n",
    " \n",
    "\n",
    "        # dLdx2 = -2 * torch.sum(comparisons, dim=0) * dB\n",
    "\n",
    "        # dLdy2 = 2 * torch.sum(comparisons, dim=1) * dW\n",
    "\n",
    " \n",
    "\n",
    "        # print('Sums')\n",
    "\n",
    "        # print(torch.all(torch.isclose(dLdx, dLdx2)))\n",
    "\n",
    "        # print(torch.all(torch.isclose(dLdy, dLdy2)))\n",
    "\n",
    "        # end code #\n",
    "\n",
    " \n",
    "\n",
    "        dL = torch.zeros_like(y_pred)\n",
    "\n",
    "        dL[B_ind] = dLdx\n",
    "\n",
    "        dL[W_ind] = dLdy\n",
    "\n",
    " \n",
    "\n",
    "        return dL\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "    def hess(self, y_true, y_pred):\n",
    "\n",
    "        B_ind = y_true >= 0.5\n",
    "\n",
    "        W_ind = y_true < 0.5\n",
    "\n",
    " \n",
    "\n",
    "        B = y_pred[B_ind]\n",
    "\n",
    "        W = y_pred[W_ind]\n",
    "\n",
    " \n",
    "\n",
    "        B_hessian = torch.zeros_like(B)\n",
    "\n",
    "        W_hessian = torch.zeros_like(W)\n",
    "\n",
    " \n",
    "\n",
    "        batch_size = 20000\n",
    "\n",
    " \n",
    "\n",
    "        for i in range(0, len(W), batch_size):\n",
    "\n",
    "            W_batch = W[i:i + batch_size]\n",
    "\n",
    " \n",
    "\n",
    "            margin_matrix_batch = W_batch.unsqueeze(1) - B.unsqueeze(0) + self.gamma\n",
    "\n",
    "            hessian_matrix_batch = torch.clamp(margin_matrix_batch, min=0)\n",
    "\n",
    " \n",
    "\n",
    "            sum_dx_batch = torch.sum(hessian_matrix_batch, dim=0)\n",
    "\n",
    "            sum_dy_batch = torch.sum(hessian_matrix_batch, dim=1)\n",
    "\n",
    " \n",
    "\n",
    "            B_hessian += -2*B*(1-B)*sum_dx_batch\n",
    "\n",
    "            B_hessian += 4*B*((1-B)**2)*sum_dx_batch\n",
    "\n",
    "            B_hessian += torch.sum(2*((B*(1-B))**2) * (hessian_matrix_batch != 0), dim=0)\n",
    "\n",
    " \n",
    "\n",
    "            W_hessian[i:i + batch_size] += 2*W_batch*(1-W_batch)*sum_dy_batch\n",
    "\n",
    "            W_hessian[i:i + batch_size] += -4*W_batch*((1-W_batch)**2)*sum_dy_batch\n",
    "\n",
    "            W_hessian[i:i + batch_size] += torch.sum(2*((W_batch*(1-W_batch))**2) * (hessian_matrix_batch != 0).T, dim=0)\n",
    "\n",
    " \n",
    "\n",
    "        # Entire Dataset\n",
    "\n",
    " \n",
    "\n",
    "        # margin_matrix = W.unsqueeze(1) - B.unsqueeze(0) + self.gamma\n",
    "\n",
    "        # hessian_matrix = torch.clamp(margin_matrix, min=0)\n",
    "\n",
    " \n",
    "\n",
    "        # sum_dx = torch.sum(hessian_matrix, dim=0)\n",
    "\n",
    "        # sum_dy = torch.sum(hessian_matrix, dim=1)\n",
    "\n",
    " \n",
    "\n",
    "        # B_hessian2 = -2*B*(1-B)*sum_dx + 4*B*((1-B)**2)*sum_dx + torch.sum(2*((B*(1-B))**2) * (hessian_matrix != 0), dim=0)\n",
    "\n",
    "        # W_hessian2 = 2*W*(1-W)*sum_dy - 4*W*((1-W)**2)*sum_dy + torch.sum(2*((W*(1-W))**2) * (hessian_matrix != 0).T, dim=0)\n",
    "\n",
    " \n",
    "\n",
    "        # end code #\n",
    "\n",
    " \n",
    "\n",
    "        # print('Hessians')\n",
    "\n",
    "        # if not torch.all(torch.isclose(B_hessian, B_hessian2, atol=1e-1)).item():\n",
    "\n",
    "        #     print(B_hessian)\n",
    "\n",
    "        #     print(B_hessian2)\n",
    "\n",
    "        # print(torch.all(torch.isclose(W_hessian, W_hessian2)))\n",
    "\n",
    " \n",
    "\n",
    "        hessians = torch.zeros_like(y_pred)\n",
    "\n",
    "        hessians[B_ind] = B_hessian\n",
    "\n",
    "        hessians[W_ind] = W_hessian\n",
    "\n",
    " \n",
    "\n",
    "        return hessians\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "    def calc_grad_hess(self, y_true, y_pred):\n",
    "\n",
    "        y_true = torch.tensor(y_true, dtype=torch.float32).cuda()\n",
    "\n",
    "        y_pred = torch.tensor(y_pred, dtype=torch.float32).cuda()\n",
    "\n",
    " \n",
    "\n",
    "        grad = self.grad(y_true, y_pred)\n",
    "\n",
    "        hess = self.hess(y_true, y_pred)\n",
    "\n",
    " \n",
    "\n",
    "        # del y_true\n",
    "\n",
    "        # del y_pred\n",
    "\n",
    " \n",
    "\n",
    "        grad = grad.cpu().detach().numpy()\n",
    "\n",
    "        hess = hess.cpu().detach().numpy()\n",
    "\n",
    " \n",
    "\n",
    "        # return grad, np.ones(grad.shape)\n",
    "\n",
    "        return grad, hess\n",
    "\n",
    " \n",
    "\n",
    "    def rocstar_obj(self, preds, train_data):\n",
    "\n",
    "        y = train_data.get_label()\n",
    "\n",
    "        p = special.expit(preds)\n",
    "\n",
    " \n",
    "\n",
    "        grad, hess = self.calc_grad_hess(y, p)\n",
    "\n",
    " \n",
    "\n",
    "        return grad, hess\n",
    "\n",
    " \n",
    "\n",
    "    def rocstar_eval(self, preds, train_data):\n",
    "\n",
    "        y = train_data.get_label()\n",
    "\n",
    "        p = special.expit(preds)\n",
    "\n",
    " \n",
    "\n",
    "        loss_metric = 'bce_loss' if self.bce_epoch > 0 else 'rocstar_loss'\n",
    "\n",
    "        loss = self.calc_loss(torch.tensor(y, dtype=torch.float32).cuda(), torch.tensor(p, dtype=torch.float32).cuda())\n",
    "\n",
    "        is_higher_better = False\n",
    "\n",
    " \n",
    "\n",
    "        return loss_metric, loss.item(), is_higher_better"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-03T03:09:27.357741Z",
     "start_time": "2024-12-03T03:09:27.336519Z"
    }
   },
   "id": "605e78ba3552d00",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_model(X_train, y_train, X_fit, X_val, y_fit, y_val):\n",
    "    fit = lgb.Dataset(X_fit, y_fit, free_raw_data=False)\n",
    "    val = lgb.Dataset(X_val, y_val, reference=fit, free_raw_data=False)\n",
    "    gamma = 0.8\n",
    "    \n",
    "    rocstar = ROCStarLoss(gamma=gamma)\n",
    "\n",
    "    params = {\n",
    "        'learning_rate': 0.03,\n",
    "        'num_leaves': 100,\n",
    "        'gamma': gamma,\n",
    "        'is_unbalance': True,\n",
    "        'objective': rocstar.rocstar_obj\n",
    "    }\n",
    "\n",
    "    print('#######   Training LightGBM with roc-star   #######')\n",
    "    model = lgb.train(\n",
    "        params=params,\n",
    "        train_set=fit,\n",
    "        num_boost_round=10000,\n",
    "        valid_sets=(fit, val),\n",
    "        valid_names=('fit', 'val'),\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=30),\n",
    "            lgb.log_evaluation(period=1)\n",
    "        ],\n",
    "        feval=rocstar.rocstar_eval,\n",
    "    )\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-03T03:09:27.923869100Z",
     "start_time": "2024-12-03T03:09:27.903754800Z"
    }
   },
   "id": "fa29d99f2a51de92",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######   Training LightGBM with roc-star   #######\n",
      "[LightGBM] [Warning] Unknown parameter: gamma\n",
      "[LightGBM] [Warning] Unknown parameter: gamma\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002256 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1303\n",
      "[LightGBM] [Info] Number of data points in the train set: 143632, number of used features: 16\n",
      "[LightGBM] [Warning] Unknown parameter: gamma\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[1]\tfit's rocstar_loss: 1.32837e+09\tval's rocstar_loss: 1.47072e+08\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[2]\tfit's rocstar_loss: 1.30106e+09\tval's rocstar_loss: 1.44307e+08\n",
      "[3]\tfit's rocstar_loss: 1.27426e+09\tval's rocstar_loss: 1.41592e+08\n",
      "[4]\tfit's rocstar_loss: 1.2478e+09\tval's rocstar_loss: 1.38931e+08\n",
      "[5]\tfit's rocstar_loss: 1.222e+09\tval's rocstar_loss: 1.36314e+08\n",
      "[6]\tfit's rocstar_loss: 1.19621e+09\tval's rocstar_loss: 1.33732e+08\n",
      "[7]\tfit's rocstar_loss: 1.16987e+09\tval's rocstar_loss: 1.31122e+08\n",
      "[8]\tfit's rocstar_loss: 1.13631e+09\tval's rocstar_loss: 1.27784e+08\n",
      "[9]\tfit's rocstar_loss: 1.11173e+09\tval's rocstar_loss: 1.25384e+08\n",
      "[10]\tfit's rocstar_loss: 1.0885e+09\tval's rocstar_loss: 1.23331e+08\n",
      "[11]\tfit's rocstar_loss: 1.07117e+09\tval's rocstar_loss: 1.21666e+08\n",
      "[12]\tfit's rocstar_loss: 1.05406e+09\tval's rocstar_loss: 1.20194e+08\n",
      "[13]\tfit's rocstar_loss: 1.0397e+09\tval's rocstar_loss: 1.19013e+08\n",
      "[14]\tfit's rocstar_loss: 1.02515e+09\tval's rocstar_loss: 1.17828e+08\n",
      "[15]\tfit's rocstar_loss: 1.01239e+09\tval's rocstar_loss: 1.16772e+08\n",
      "[16]\tfit's rocstar_loss: 1.00171e+09\tval's rocstar_loss: 1.15955e+08\n",
      "[17]\tfit's rocstar_loss: 9.91833e+08\tval's rocstar_loss: 1.15095e+08\n",
      "[18]\tfit's rocstar_loss: 9.8302e+08\tval's rocstar_loss: 1.14393e+08\n",
      "[19]\tfit's rocstar_loss: 9.74973e+08\tval's rocstar_loss: 1.13618e+08\n",
      "[20]\tfit's rocstar_loss: 9.66914e+08\tval's rocstar_loss: 1.13084e+08\n",
      "[21]\tfit's rocstar_loss: 9.59608e+08\tval's rocstar_loss: 1.12473e+08\n",
      "[22]\tfit's rocstar_loss: 9.52167e+08\tval's rocstar_loss: 1.11945e+08\n",
      "[23]\tfit's rocstar_loss: 9.44704e+08\tval's rocstar_loss: 1.11484e+08\n",
      "[24]\tfit's rocstar_loss: 9.38233e+08\tval's rocstar_loss: 1.1111e+08\n",
      "[25]\tfit's rocstar_loss: 9.31472e+08\tval's rocstar_loss: 1.10627e+08\n",
      "[26]\tfit's rocstar_loss: 9.25791e+08\tval's rocstar_loss: 1.10198e+08\n",
      "[27]\tfit's rocstar_loss: 9.20819e+08\tval's rocstar_loss: 1.09861e+08\n",
      "[28]\tfit's rocstar_loss: 9.15409e+08\tval's rocstar_loss: 1.09736e+08\n",
      "[29]\tfit's rocstar_loss: 9.1046e+08\tval's rocstar_loss: 1.09546e+08\n",
      "[30]\tfit's rocstar_loss: 9.05964e+08\tval's rocstar_loss: 1.09458e+08\n",
      "[31]\tfit's rocstar_loss: 9.01091e+08\tval's rocstar_loss: 1.09216e+08\n",
      "[32]\tfit's rocstar_loss: 8.97235e+08\tval's rocstar_loss: 1.0899e+08\n",
      "[33]\tfit's rocstar_loss: 8.93618e+08\tval's rocstar_loss: 1.08885e+08\n",
      "[34]\tfit's rocstar_loss: 8.90336e+08\tval's rocstar_loss: 1.08731e+08\n",
      "[35]\tfit's rocstar_loss: 8.85914e+08\tval's rocstar_loss: 1.08532e+08\n",
      "[36]\tfit's rocstar_loss: 8.82248e+08\tval's rocstar_loss: 1.08395e+08\n",
      "[37]\tfit's rocstar_loss: 8.78788e+08\tval's rocstar_loss: 1.08166e+08\n",
      "[38]\tfit's rocstar_loss: 8.75456e+08\tval's rocstar_loss: 1.08027e+08\n",
      "[39]\tfit's rocstar_loss: 8.71911e+08\tval's rocstar_loss: 1.07995e+08\n",
      "[40]\tfit's rocstar_loss: 8.68445e+08\tval's rocstar_loss: 1.07873e+08\n",
      "[41]\tfit's rocstar_loss: 8.65292e+08\tval's rocstar_loss: 1.07696e+08\n",
      "[42]\tfit's rocstar_loss: 8.6187e+08\tval's rocstar_loss: 1.07401e+08\n",
      "[43]\tfit's rocstar_loss: 8.59489e+08\tval's rocstar_loss: 1.07404e+08\n",
      "[44]\tfit's rocstar_loss: 8.56318e+08\tval's rocstar_loss: 1.07357e+08\n",
      "[45]\tfit's rocstar_loss: 8.53009e+08\tval's rocstar_loss: 1.07182e+08\n",
      "[46]\tfit's rocstar_loss: 8.50043e+08\tval's rocstar_loss: 1.06991e+08\n",
      "[47]\tfit's rocstar_loss: 8.47478e+08\tval's rocstar_loss: 1.0694e+08\n",
      "[48]\tfit's rocstar_loss: 8.4456e+08\tval's rocstar_loss: 1.06856e+08\n",
      "[49]\tfit's rocstar_loss: 8.42274e+08\tval's rocstar_loss: 1.06759e+08\n",
      "[50]\tfit's rocstar_loss: 8.40081e+08\tval's rocstar_loss: 1.06694e+08\n",
      "[51]\tfit's rocstar_loss: 8.37846e+08\tval's rocstar_loss: 1.06624e+08\n",
      "[52]\tfit's rocstar_loss: 8.35428e+08\tval's rocstar_loss: 1.06537e+08\n",
      "[53]\tfit's rocstar_loss: 8.32842e+08\tval's rocstar_loss: 1.06467e+08\n",
      "[54]\tfit's rocstar_loss: 8.30598e+08\tval's rocstar_loss: 1.06437e+08\n",
      "[55]\tfit's rocstar_loss: 8.27982e+08\tval's rocstar_loss: 1.06341e+08\n",
      "[56]\tfit's rocstar_loss: 8.25744e+08\tval's rocstar_loss: 1.06268e+08\n",
      "[57]\tfit's rocstar_loss: 8.2373e+08\tval's rocstar_loss: 1.0614e+08\n",
      "[58]\tfit's rocstar_loss: 8.21782e+08\tval's rocstar_loss: 1.06101e+08\n",
      "[59]\tfit's rocstar_loss: 8.19878e+08\tval's rocstar_loss: 1.06061e+08\n",
      "[60]\tfit's rocstar_loss: 8.17983e+08\tval's rocstar_loss: 1.06033e+08\n",
      "[61]\tfit's rocstar_loss: 8.15869e+08\tval's rocstar_loss: 1.05994e+08\n",
      "[62]\tfit's rocstar_loss: 8.14075e+08\tval's rocstar_loss: 1.05959e+08\n",
      "[63]\tfit's rocstar_loss: 8.1238e+08\tval's rocstar_loss: 1.05981e+08\n",
      "[64]\tfit's rocstar_loss: 8.10726e+08\tval's rocstar_loss: 1.05949e+08\n",
      "[65]\tfit's rocstar_loss: 8.08876e+08\tval's rocstar_loss: 1.05989e+08\n",
      "[66]\tfit's rocstar_loss: 8.07233e+08\tval's rocstar_loss: 1.05941e+08\n",
      "[67]\tfit's rocstar_loss: 8.05499e+08\tval's rocstar_loss: 1.05931e+08\n",
      "[68]\tfit's rocstar_loss: 8.03822e+08\tval's rocstar_loss: 1.05937e+08\n",
      "[69]\tfit's rocstar_loss: 8.02141e+08\tval's rocstar_loss: 1.05955e+08\n",
      "[70]\tfit's rocstar_loss: 8.00298e+08\tval's rocstar_loss: 1.05963e+08\n",
      "[71]\tfit's rocstar_loss: 7.98465e+08\tval's rocstar_loss: 1.05891e+08\n",
      "[72]\tfit's rocstar_loss: 7.96362e+08\tval's rocstar_loss: 1.058e+08\n",
      "[73]\tfit's rocstar_loss: 7.94471e+08\tval's rocstar_loss: 1.0589e+08\n",
      "[74]\tfit's rocstar_loss: 7.92843e+08\tval's rocstar_loss: 1.05885e+08\n",
      "[75]\tfit's rocstar_loss: 7.91084e+08\tval's rocstar_loss: 1.05879e+08\n",
      "[76]\tfit's rocstar_loss: 7.89245e+08\tval's rocstar_loss: 1.05821e+08\n",
      "[77]\tfit's rocstar_loss: 7.87475e+08\tval's rocstar_loss: 1.05877e+08\n",
      "[78]\tfit's rocstar_loss: 7.85666e+08\tval's rocstar_loss: 1.05964e+08\n",
      "[79]\tfit's rocstar_loss: 7.84093e+08\tval's rocstar_loss: 1.0594e+08\n",
      "[80]\tfit's rocstar_loss: 7.82426e+08\tval's rocstar_loss: 1.05898e+08\n",
      "[81]\tfit's rocstar_loss: 7.80628e+08\tval's rocstar_loss: 1.05864e+08\n",
      "[82]\tfit's rocstar_loss: 7.7857e+08\tval's rocstar_loss: 1.0581e+08\n",
      "[83]\tfit's rocstar_loss: 7.76546e+08\tval's rocstar_loss: 1.0587e+08\n",
      "[84]\tfit's rocstar_loss: 7.74708e+08\tval's rocstar_loss: 1.05838e+08\n",
      "[85]\tfit's rocstar_loss: 7.72787e+08\tval's rocstar_loss: 1.05818e+08\n",
      "[86]\tfit's rocstar_loss: 7.70838e+08\tval's rocstar_loss: 1.05996e+08\n",
      "[87]\tfit's rocstar_loss: 7.69041e+08\tval's rocstar_loss: 1.06029e+08\n",
      "[88]\tfit's rocstar_loss: 7.67354e+08\tval's rocstar_loss: 1.06048e+08\n",
      "[89]\tfit's rocstar_loss: 7.65498e+08\tval's rocstar_loss: 1.06076e+08\n",
      "[90]\tfit's rocstar_loss: 7.6377e+08\tval's rocstar_loss: 1.06047e+08\n",
      "[91]\tfit's rocstar_loss: 7.62202e+08\tval's rocstar_loss: 1.06058e+08\n",
      "[92]\tfit's rocstar_loss: 7.60454e+08\tval's rocstar_loss: 1.06062e+08\n",
      "[93]\tfit's rocstar_loss: 7.58814e+08\tval's rocstar_loss: 1.0607e+08\n",
      "[94]\tfit's rocstar_loss: 7.57018e+08\tval's rocstar_loss: 1.0615e+08\n",
      "[95]\tfit's rocstar_loss: 7.55522e+08\tval's rocstar_loss: 1.06185e+08\n",
      "[96]\tfit's rocstar_loss: 7.53847e+08\tval's rocstar_loss: 1.06148e+08\n",
      "[97]\tfit's rocstar_loss: 7.522e+08\tval's rocstar_loss: 1.06125e+08\n",
      "[98]\tfit's rocstar_loss: 7.50731e+08\tval's rocstar_loss: 1.06142e+08\n",
      "[99]\tfit's rocstar_loss: 7.49126e+08\tval's rocstar_loss: 1.06155e+08\n",
      "[100]\tfit's rocstar_loss: 7.4755e+08\tval's rocstar_loss: 1.06128e+08\n",
      "[101]\tfit's rocstar_loss: 7.45821e+08\tval's rocstar_loss: 1.0614e+08\n",
      "[102]\tfit's rocstar_loss: 7.44107e+08\tval's rocstar_loss: 1.06156e+08\n",
      "Early stopping, best iteration is:\n",
      "[72]\tfit's rocstar_loss: 7.96362e+08\tval's rocstar_loss: 1.058e+08\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_fit, X_val, y_fit, y_val = train_test_split(\n",
    "    X_train, y_train,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rocstar_model = train_model(X_train, y_train, X_fit, X_val, y_fit, y_val)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-03T03:30:45.076834500Z",
     "start_time": "2024-12-03T03:09:28.880862200Z"
    }
   },
   "id": "fb5f515b5b5da519",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def predict_results(model, X_test, y_test):\n",
    "    y_pred = special.expit(model.predict(X_test))\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "\n",
    "    optimal_idx = (tpr - fpr).argmax()\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    \n",
    "    y_pred_binary = (y_pred >= optimal_threshold).astype(int)\n",
    "    # y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "    print(f\"Test's ROC AUC: {roc_auc_score(y_test, y_pred):.5f}\")\n",
    "    print(f\"Test's logloss: {log_loss(y_test, y_pred):.5f}\")\n",
    "\n",
    "    return y_pred, y_pred_binary"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-03T03:30:45.109613Z",
     "start_time": "2024-12-03T03:30:45.077924900Z"
    }
   },
   "id": "b401907cb33a3288",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test's ROC AUC: 0.74620\n",
      "Test's logloss: 0.86630\n"
     ]
    }
   ],
   "source": [
    "y_pred, y_pred_binary = predict_results(rocstar_model, X_test, y_test)\n",
    "\n",
    "# binary_predictions = pd.DataFrame({\"y_true\": y_test, \"y_pred\": y_pred_binary})\n",
    "# binary_predictions.to_csv(\"artifacts/predictions/lightgbm_rocstar_predictions.csv\", index=False)\n",
    "\n",
    "predictions = pd.DataFrame({\"y_true\": y_test, \"y_pred\": y_pred})\n",
    "predictions.to_csv(\"artifacts/lightgbm_preds/loan/lightgbm_rocstar_continuous_predictions.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-03T03:30:45.433597100Z",
     "start_time": "2024-12-03T03:30:45.098700500Z"
    }
   },
   "id": "1470ed1b55dcf6e9",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-28T20:24:38.897428300Z",
     "start_time": "2024-11-28T20:24:38.882651400Z"
    }
   },
   "id": "1a4f76299086f81c",
   "execution_count": 111
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-28T21:23:49.814360200Z",
     "start_time": "2024-11-28T21:23:49.056281800Z"
    }
   },
   "id": "ec6919e346878140",
   "execution_count": 131
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c7394cb47979787c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
