{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-28T20:06:55.024502800Z",
     "start_time": "2024-11-28T20:06:55.003930Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve, accuracy_score, roc_auc_score, log_loss\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from src.data_prep import prepare_data\n",
    "# from src.models import logistic_regression_model, decision_tree_model, random_forest_model, lightgbm_model\n",
    "# from src.train import train_sklearn_model\n",
    "from src.evaluation import evaluate_model\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import joblib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "            Time        V1        V2        V3        V4        V5        V6  \\\n223361  143352.0  1.955041 -0.380783 -0.315013  0.330155 -0.509374 -0.086197   \n165061  117173.0 -0.400975 -0.626943  1.555339 -2.017772 -0.107769  0.168310   \n238186  149565.0  0.072509  0.820566 -0.561351 -0.709897  1.080399 -0.359429   \n150562   93670.0 -0.535045  1.014587  1.750679  2.769390  0.500089  1.002270   \n138452   82655.0 -4.026938  1.897371 -0.429786 -0.029571 -0.855751 -0.480406   \n\n              V7        V8        V9  ...       V20       V21       V22  \\\n223361 -0.627978  0.035994  1.054560  ... -0.125390  0.238197  0.968305   \n165061  0.017959 -0.401619  0.040378  ... -0.470372 -0.153485  0.421703   \n238186  0.787858  0.117276 -0.131275  ...  0.012227 -0.314638 -0.872959   \n150562  0.847902 -0.081323  0.371579  ... -0.253757  0.063525  0.443431   \n138452 -0.435632  1.313760  0.536044  ... -0.012320 -0.480691 -0.230369   \n\n             V23       V24       V25       V26       V27       V28  Amount  \n223361  0.053208 -0.278602 -0.044999 -0.216780  0.045168 -0.047145    9.99  \n165061  0.113442 -1.004095 -1.176695  0.361924 -0.370469 -0.144792   45.90  \n238186  0.083391  0.148178 -0.431459  0.119690  0.206395  0.070288   11.99  \n150562 -0.072754  0.448192 -0.655203 -0.181038 -0.093013 -0.064931  117.44  \n138452  0.250717  0.066399  0.470787  0.245335  0.286904 -0.322672   25.76  \n\n[5 rows x 30 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V20</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>223361</th>\n      <td>143352.0</td>\n      <td>1.955041</td>\n      <td>-0.380783</td>\n      <td>-0.315013</td>\n      <td>0.330155</td>\n      <td>-0.509374</td>\n      <td>-0.086197</td>\n      <td>-0.627978</td>\n      <td>0.035994</td>\n      <td>1.054560</td>\n      <td>...</td>\n      <td>-0.125390</td>\n      <td>0.238197</td>\n      <td>0.968305</td>\n      <td>0.053208</td>\n      <td>-0.278602</td>\n      <td>-0.044999</td>\n      <td>-0.216780</td>\n      <td>0.045168</td>\n      <td>-0.047145</td>\n      <td>9.99</td>\n    </tr>\n    <tr>\n      <th>165061</th>\n      <td>117173.0</td>\n      <td>-0.400975</td>\n      <td>-0.626943</td>\n      <td>1.555339</td>\n      <td>-2.017772</td>\n      <td>-0.107769</td>\n      <td>0.168310</td>\n      <td>0.017959</td>\n      <td>-0.401619</td>\n      <td>0.040378</td>\n      <td>...</td>\n      <td>-0.470372</td>\n      <td>-0.153485</td>\n      <td>0.421703</td>\n      <td>0.113442</td>\n      <td>-1.004095</td>\n      <td>-1.176695</td>\n      <td>0.361924</td>\n      <td>-0.370469</td>\n      <td>-0.144792</td>\n      <td>45.90</td>\n    </tr>\n    <tr>\n      <th>238186</th>\n      <td>149565.0</td>\n      <td>0.072509</td>\n      <td>0.820566</td>\n      <td>-0.561351</td>\n      <td>-0.709897</td>\n      <td>1.080399</td>\n      <td>-0.359429</td>\n      <td>0.787858</td>\n      <td>0.117276</td>\n      <td>-0.131275</td>\n      <td>...</td>\n      <td>0.012227</td>\n      <td>-0.314638</td>\n      <td>-0.872959</td>\n      <td>0.083391</td>\n      <td>0.148178</td>\n      <td>-0.431459</td>\n      <td>0.119690</td>\n      <td>0.206395</td>\n      <td>0.070288</td>\n      <td>11.99</td>\n    </tr>\n    <tr>\n      <th>150562</th>\n      <td>93670.0</td>\n      <td>-0.535045</td>\n      <td>1.014587</td>\n      <td>1.750679</td>\n      <td>2.769390</td>\n      <td>0.500089</td>\n      <td>1.002270</td>\n      <td>0.847902</td>\n      <td>-0.081323</td>\n      <td>0.371579</td>\n      <td>...</td>\n      <td>-0.253757</td>\n      <td>0.063525</td>\n      <td>0.443431</td>\n      <td>-0.072754</td>\n      <td>0.448192</td>\n      <td>-0.655203</td>\n      <td>-0.181038</td>\n      <td>-0.093013</td>\n      <td>-0.064931</td>\n      <td>117.44</td>\n    </tr>\n    <tr>\n      <th>138452</th>\n      <td>82655.0</td>\n      <td>-4.026938</td>\n      <td>1.897371</td>\n      <td>-0.429786</td>\n      <td>-0.029571</td>\n      <td>-0.855751</td>\n      <td>-0.480406</td>\n      <td>-0.435632</td>\n      <td>1.313760</td>\n      <td>0.536044</td>\n      <td>...</td>\n      <td>-0.012320</td>\n      <td>-0.480691</td>\n      <td>-0.230369</td>\n      <td>0.250717</td>\n      <td>0.066399</td>\n      <td>0.470787</td>\n      <td>0.245335</td>\n      <td>0.286904</td>\n      <td>-0.322672</td>\n      <td>25.76</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 30 columns</p>\n</div>"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/creditcard/creditcard.csv')\n",
    "X = df.drop(columns='Class')\n",
    "y = df['Class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    random_state=42,\n",
    "    test_size=0.2\n",
    ")\n",
    "\n",
    "X_fit, X_val, y_fit, y_val = train_test_split(\n",
    "    X_train, y_train,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-28T20:23:11.707866500Z",
     "start_time": "2024-11-28T20:23:09.074461Z"
    }
   },
   "id": "304a2f19fb78401d",
   "execution_count": 101
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 316, number of negative: 170567\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011132 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 170883, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.001849 -> initscore=-6.291141\n",
      "[LightGBM] [Info] Start training from score -6.291141\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tfit's binary_logloss: 0.00197699\tval's binary_logloss: 0.00309055\n",
      "[200]\tfit's binary_logloss: 0.000865919\tval's binary_logloss: 0.00252994\n",
      "[300]\tfit's binary_logloss: 0.000432025\tval's binary_logloss: 0.00238922\n",
      "Early stopping, best iteration is:\n",
      "[355]\tfit's binary_logloss: 0.000304252\tval's binary_logloss: 0.00235661\n",
      "\n",
      "Test's ROC AUC: 0.98114\n",
      "Test's logloss: 0.00250\n"
     ]
    }
   ],
   "source": [
    "fit = lgb.Dataset(X_fit, y_fit)\n",
    "val = lgb.Dataset(X_val, y_val, reference=fit)\n",
    "\n",
    "model = lgb.train(\n",
    "    params={\n",
    "        'learning_rate': 0.01,\n",
    "        'objective': 'binary'\n",
    "    },\n",
    "    train_set=fit,\n",
    "    num_boost_round=400,\n",
    "    valid_sets=(fit, val),\n",
    "    valid_names=('fit', 'val'),\n",
    "  callbacks=[\n",
    "    lgb.early_stopping(stopping_rounds=20),\n",
    "    lgb.log_evaluation(period=100)\n",
    "    ]\n",
    ")\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print()\n",
    "print(f\"Test's ROC AUC: {roc_auc_score(y_test, y_pred):.5f}\")\n",
    "print(f\"Test's logloss: {log_loss(y_test, y_pred):.5f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-28T20:23:17.764213400Z",
     "start_time": "2024-11-28T20:23:14.809929200Z"
    }
   },
   "id": "7ab3919d456bbeea",
   "execution_count": 102
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary predictions saved successfully!\n"
     ]
    }
   ],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "\n",
    "optimal_idx = (tpr - fpr).argmax()\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "y_pred_binary = (y_pred >= optimal_threshold).astype(int)\n",
    "# y_pred_binary = (y_pred >= 0.5).astype(int)\n",
    "\n",
    "# Save binary predictions and true labels\n",
    "binary_predictions = pd.DataFrame({\"y_true\": y_test, \"y_pred\": y_pred_binary})\n",
    "binary_predictions.to_csv(\"artifacts/predictions/lightgbm_bce_predictions.csv\", index=False)\n",
    "\n",
    "predictions = pd.DataFrame({\"y_true\": y_test, \"y_pred\": y_pred})\n",
    "predictions.to_csv(\"artifacts/lightgbm_preds/lightgbm_bce_continuous_predictions.csv\", index=False)\n",
    "\n",
    "# print(f\"Optimal Threshold: {optimal_threshold:.5f}\")\n",
    "print(\"Binary predictions saved successfully!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-28T20:23:22.290732300Z",
     "start_time": "2024-11-28T20:23:22.135978100Z"
    }
   },
   "id": "475ac7123eb26889",
   "execution_count": 103
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize\n",
    "from scipy import special\n",
    "\n",
    "class FocalLoss:\n",
    "\n",
    "    def __init__(self, gamma, alpha=None):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def at(self, y):\n",
    "        if self.alpha is None:\n",
    "            return np.ones_like(y)\n",
    "        return np.where(y, self.alpha, 1 - self.alpha)\n",
    "\n",
    "    def pt(self, y, p):\n",
    "        p = np.clip(p, 1e-15, 1 - 1e-15)\n",
    "        return np.where(y, p, 1 - p)\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        at = self.at(y_true)\n",
    "        pt = self.pt(y_true, y_pred)\n",
    "        return -at * (1 - pt) ** self.gamma * np.log(pt)\n",
    "\n",
    "    def grad(self, y_true, y_pred):\n",
    "        y = 2 * y_true - 1  # {0, 1} -> {-1, 1}\n",
    "        at = self.at(y_true)\n",
    "        pt = self.pt(y_true, y_pred)\n",
    "        g = self.gamma\n",
    "        return at * y * (1 - pt) ** g * (g * pt * np.log(pt) + pt - 1)\n",
    "\n",
    "    def hess(self, y_true, y_pred):\n",
    "        y = 2 * y_true - 1  # {0, 1} -> {-1, 1}\n",
    "        at = self.at(y_true)\n",
    "        pt = self.pt(y_true, y_pred)\n",
    "        g = self.gamma\n",
    "\n",
    "        u = at * y * (1 - pt) ** g\n",
    "        du = -at * y * g * (1 - pt) ** (g - 1)\n",
    "        v = g * pt * np.log(pt) + pt - 1\n",
    "        dv = g * np.log(pt) + g + 1\n",
    "\n",
    "        return (du * v + u * dv) * y * (pt * (1 - pt))\n",
    "\n",
    "    def init_score(self, y_true):\n",
    "        res = optimize.minimize_scalar(\n",
    "            lambda p: self(y_true, p).sum(),\n",
    "            bounds=(0, 1),\n",
    "            method='bounded'\n",
    "        )\n",
    "        p = res.x\n",
    "        log_odds = np.log(p / (1 - p))\n",
    "        return log_odds\n",
    "\n",
    "    def lgb_obj(self, preds, train_data):\n",
    "        y = train_data.get_label()\n",
    "        p = special.expit(preds)\n",
    "        return self.grad(y, p), self.hess(y, p)\n",
    "\n",
    "    def lgb_eval(self, preds, train_data):\n",
    "        y = train_data.get_label()\n",
    "        p = special.expit(preds)\n",
    "        is_higher_better = False\n",
    "        return 'focal_loss', self(y, p).mean(), is_higher_better"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-28T20:23:23.589178400Z",
     "start_time": "2024-11-28T20:23:23.563166800Z"
    }
   },
   "id": "39d80f1b41e49891",
   "execution_count": 104
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016844 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 170883, number of used features: 30\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tfit's focal_loss: 0.00198357\tval's focal_loss: 0.00309224\n",
      "[200]\tfit's focal_loss: 0.000867472\tval's focal_loss: 0.00253809\n",
      "[300]\tfit's focal_loss: 0.000433516\tval's focal_loss: 0.00239292\n",
      "Early stopping, best iteration is:\n",
      "[364]\tfit's focal_loss: 0.000287925\tval's focal_loss: 0.00236993\n",
      "\n",
      "Test's ROC AUC: 0.98151\n",
      "Test's logloss: 0.00249\n"
     ]
    }
   ],
   "source": [
    "fl = FocalLoss(alpha=None, gamma=0)\n",
    "\n",
    "fit = lgb.Dataset(\n",
    "    X_fit, y_fit,\n",
    "    init_score=np.full_like(y_fit, fl.init_score(y_fit), dtype=float)\n",
    ")\n",
    "\n",
    "val = lgb.Dataset(\n",
    "    X_val, y_val,\n",
    "    init_score=np.full_like(y_val, fl.init_score(y_fit), dtype=float),\n",
    "    reference=fit\n",
    ")\n",
    "\n",
    "model = lgb.train(\n",
    "    params={\n",
    "        'learning_rate': 0.01,\n",
    "        'objective': fl.lgb_obj\n",
    "    },\n",
    "    train_set=fit,\n",
    "    num_boost_round=10000,\n",
    "    valid_sets=(fit, val),\n",
    "    valid_names=('fit', 'val'),\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=20),\n",
    "        lgb.log_evaluation(period=100)\n",
    "    ],\n",
    "    feval=fl.lgb_eval\n",
    ")\n",
    "\n",
    "y_pred = special.expit(fl.init_score(y_fit) + model.predict(X_test))\n",
    "\n",
    "print()\n",
    "print(f\"Test's ROC AUC: {roc_auc_score(y_test, y_pred):.5f}\")\n",
    "print(f\"Test's logloss: {log_loss(y_test, y_pred):.5f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-28T20:24:06.732686900Z",
     "start_time": "2024-11-28T20:23:24.839472800Z"
    }
   },
   "id": "40026623f755db8d",
   "execution_count": 105
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Threshold: 0.00044\n",
      "Binary predictions saved successfully!\n"
     ]
    }
   ],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "\n",
    "optimal_idx = (tpr - fpr).argmax()\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "y_pred_binary = (y_pred >= optimal_threshold).astype(int)\n",
    "\n",
    "# y_pred_binary = (y_pred >= 0.5).astype(int)\n",
    "\n",
    "# binary_predictions = pd.DataFrame({\"y_true\": y_test, \"y_pred\": y_pred_binary})\n",
    "# binary_predictions.to_csv(\"artifacts/predictions/lightgbm_focal_loss_predictions.csv\", index=False)\n",
    "\n",
    "predictions = pd.DataFrame({\"y_true\": y_test, \"y_pred\": y_pred})\n",
    "predictions.to_csv(\"artifacts/lightgbm_preds/lightgbm_focal_loss_continuous_predictions.csv\", index=False)\n",
    "\n",
    "print(f\"Optimal Threshold: {optimal_threshold:.5f}\")\n",
    "print(\"Binary predictions saved successfully!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-28T20:24:06.874626400Z",
     "start_time": "2024-11-28T20:24:06.737784300Z"
    }
   },
   "id": "f4c42b6865dfb794",
   "execution_count": 106
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ROC-Star"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6b7a47f3d3ca3d7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ROCStarLoss:\n",
    "\n",
    "    def __init__(self, delta=2, gamma=0.4):\n",
    "\n",
    "        self.delta = delta\n",
    "\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.epoch_true = None\n",
    "\n",
    "        self.epoch_pred = None\n",
    "\n",
    "        self.bce_epoch = 0\n",
    "\n",
    "        self.BCE = nn.BCELoss()\n",
    "\n",
    " \n",
    "\n",
    "    def calc_loss(self, y_true, y_pred):\n",
    "\n",
    "       \n",
    "\n",
    "        # If first epoch, BCE Loss\n",
    "\n",
    "        if self.bce_epoch > 0:\n",
    "\n",
    "            self.bce_epoch -= 1\n",
    "\n",
    "            self.epoch_true = y_true.clone()\n",
    "\n",
    "            self.epoch_pred = y_pred.clone()\n",
    "\n",
    " \n",
    "\n",
    "            return self.BCE(y_pred, y_true)\n",
    "\n",
    " \n",
    "\n",
    "        # B/W cmparison for Appeal/No Appeal - [B(+), W(-)]\n",
    "\n",
    " \n",
    "\n",
    "        pos_ind = y_true >= 0.5\n",
    "\n",
    "        neg_ind = y_true < 0.5\n",
    "\n",
    " \n",
    "\n",
    "        B = y_pred[pos_ind]\n",
    "\n",
    "        W = y_pred[neg_ind]\n",
    "\n",
    " \n",
    "\n",
    "        B_shifted = B - self.gamma\n",
    "\n",
    " \n",
    "\n",
    "        # Batch-wise loss calculatoin\n",
    "\n",
    "        batch_size = 50000\n",
    "\n",
    "        loss = 0.0\n",
    "\n",
    " \n",
    "\n",
    "        for i in range(0, len(W), batch_size):\n",
    "\n",
    "            W_batch = W[i:i + batch_size]\n",
    "\n",
    " \n",
    "\n",
    "            comparisons_batch = W_batch.unsqueeze(1) - B_shifted.unsqueeze(0)\n",
    "\n",
    " \n",
    "\n",
    "            loss += torch.sum((torch.clamp(comparisons_batch, min=0))**2)\n",
    "\n",
    " \n",
    "\n",
    "        # end code #\n",
    "\n",
    " \n",
    "\n",
    "        # full dataset loss\n",
    "\n",
    "        # comparisons = W.unsqueeze(1) - B_shifted.unsqueeze(0)\n",
    "\n",
    " \n",
    "\n",
    "        # loss_matrix = torch.clamp(comparisons, min=0)\n",
    "\n",
    "        # loss2 = torch.sum(loss_matrix ** 2)\n",
    "\n",
    "        # print(\"Losses for full and batch-wise\")\n",
    "\n",
    "        # print(loss)\n",
    "\n",
    "        # print(loss2)\n",
    "\n",
    "        # end code #\n",
    "\n",
    " \n",
    "\n",
    "        return loss\n",
    "\n",
    " \n",
    "\n",
    "    def init_score(self, y_true):\n",
    "\n",
    "        p = np.mean(y_true)\n",
    "\n",
    "        p = np.clip(p, 1e-15, 1 - 1e-15)\n",
    "\n",
    "        log_odds = np.log(p / (1 - p))\n",
    "\n",
    "        return log_odds\n",
    "\n",
    " \n",
    "\n",
    "    def grad(self, y_true, y_pred):\n",
    "\n",
    "        B_ind = y_true >= 0.5\n",
    "\n",
    "        W_ind = y_true < 0.5\n",
    "\n",
    " \n",
    "\n",
    "        B = y_pred[B_ind]\n",
    "\n",
    "        W = y_pred[W_ind]\n",
    "\n",
    " \n",
    "\n",
    "        dB = B * (1 - B)\n",
    "\n",
    "        dW = W * (1 - W)\n",
    "\n",
    " \n",
    "\n",
    "        # Batching for GPU Memory saving\n",
    "\n",
    "        batch_size = 50000\n",
    "\n",
    " \n",
    "\n",
    "        sum_comparisons_B = torch.zeros_like(B)\n",
    "\n",
    "        sum_comparisons_W = torch.zeros_like(W)\n",
    "\n",
    " \n",
    "\n",
    "        for i in range(0, len(W), batch_size):\n",
    "\n",
    "            W_batch = W[i: i + batch_size]\n",
    "\n",
    " \n",
    "\n",
    "            comparisons_B_batch = W_batch.unsqueeze(1) - B.unsqueeze(0) + self.gamma\n",
    "\n",
    " \n",
    "\n",
    "            sum_comparisons_B_batch = torch.clamp(comparisons_B_batch, min=0).sum(dim=0)\n",
    "\n",
    "            sum_comparisons_W_batch = torch.clamp(comparisons_B_batch, min=0).sum(dim=1)\n",
    "\n",
    " \n",
    "\n",
    "            sum_comparisons_B += sum_comparisons_B_batch\n",
    "\n",
    "            sum_comparisons_W[i:i + batch_size] += sum_comparisons_W_batch\n",
    "\n",
    " \n",
    "\n",
    "        dLdx = -2 * sum_comparisons_B * dB\n",
    "\n",
    "        dLdy = 2 * sum_comparisons_W * dW\n",
    "\n",
    " \n",
    "\n",
    "        # end code #\n",
    "\n",
    " \n",
    "\n",
    "        # Normal matrix code for full dataset\n",
    "\n",
    "        # comparisons = W.unsqueeze(1) - B.unsqueeze(0) + self.gamma\n",
    "\n",
    "        # comparisons = torch.clamp(comparisons, min=0)\n",
    "\n",
    " \n",
    "\n",
    "        # dLdx2 = -2 * torch.sum(comparisons, dim=0) * dB\n",
    "\n",
    "        # dLdy2 = 2 * torch.sum(comparisons, dim=1) * dW\n",
    "\n",
    " \n",
    "\n",
    "        # print('Sums')\n",
    "\n",
    "        # print(torch.all(torch.isclose(dLdx, dLdx2)))\n",
    "\n",
    "        # print(torch.all(torch.isclose(dLdy, dLdy2)))\n",
    "\n",
    "        # end code #\n",
    "\n",
    " \n",
    "\n",
    "        dL = torch.zeros_like(y_pred)\n",
    "\n",
    "        dL[B_ind] = dLdx\n",
    "\n",
    "        dL[W_ind] = dLdy\n",
    "\n",
    " \n",
    "\n",
    "        return dL\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "    def hess(self, y_true, y_pred):\n",
    "\n",
    "        B_ind = y_true >= 0.5\n",
    "\n",
    "        W_ind = y_true < 0.5\n",
    "\n",
    " \n",
    "\n",
    "        B = y_pred[B_ind]\n",
    "\n",
    "        W = y_pred[W_ind]\n",
    "\n",
    " \n",
    "\n",
    "        B_hessian = torch.zeros_like(B)\n",
    "\n",
    "        W_hessian = torch.zeros_like(W)\n",
    "\n",
    " \n",
    "\n",
    "        batch_size = 20000\n",
    "\n",
    " \n",
    "\n",
    "        for i in range(0, len(W), batch_size):\n",
    "\n",
    "            W_batch = W[i:i + batch_size]\n",
    "\n",
    " \n",
    "\n",
    "            margin_matrix_batch = W_batch.unsqueeze(1) - B.unsqueeze(0) + self.gamma\n",
    "\n",
    "            hessian_matrix_batch = torch.clamp(margin_matrix_batch, min=0)\n",
    "\n",
    " \n",
    "\n",
    "            sum_dx_batch = torch.sum(hessian_matrix_batch, dim=0)\n",
    "\n",
    "            sum_dy_batch = torch.sum(hessian_matrix_batch, dim=1)\n",
    "\n",
    " \n",
    "\n",
    "            B_hessian += -2*B*(1-B)*sum_dx_batch\n",
    "\n",
    "            B_hessian += 4*B*((1-B)**2)*sum_dx_batch\n",
    "\n",
    "            B_hessian += torch.sum(2*((B*(1-B))**2) * (hessian_matrix_batch != 0), dim=0)\n",
    "\n",
    " \n",
    "\n",
    "            W_hessian[i:i + batch_size] += 2*W_batch*(1-W_batch)*sum_dy_batch\n",
    "\n",
    "            W_hessian[i:i + batch_size] += -4*W_batch*((1-W_batch)**2)*sum_dy_batch\n",
    "\n",
    "            W_hessian[i:i + batch_size] += torch.sum(2*((W_batch*(1-W_batch))**2) * (hessian_matrix_batch != 0).T, dim=0)\n",
    "\n",
    " \n",
    "\n",
    "        # Entire Dataset\n",
    "\n",
    " \n",
    "\n",
    "        # margin_matrix = W.unsqueeze(1) - B.unsqueeze(0) + self.gamma\n",
    "\n",
    "        # hessian_matrix = torch.clamp(margin_matrix, min=0)\n",
    "\n",
    " \n",
    "\n",
    "        # sum_dx = torch.sum(hessian_matrix, dim=0)\n",
    "\n",
    "        # sum_dy = torch.sum(hessian_matrix, dim=1)\n",
    "\n",
    " \n",
    "\n",
    "        # B_hessian2 = -2*B*(1-B)*sum_dx + 4*B*((1-B)**2)*sum_dx + torch.sum(2*((B*(1-B))**2) * (hessian_matrix != 0), dim=0)\n",
    "\n",
    "        # W_hessian2 = 2*W*(1-W)*sum_dy - 4*W*((1-W)**2)*sum_dy + torch.sum(2*((W*(1-W))**2) * (hessian_matrix != 0).T, dim=0)\n",
    "\n",
    " \n",
    "\n",
    "        # end code #\n",
    "\n",
    " \n",
    "\n",
    "        # print('Hessians')\n",
    "\n",
    "        # if not torch.all(torch.isclose(B_hessian, B_hessian2, atol=1e-1)).item():\n",
    "\n",
    "        #     print(B_hessian)\n",
    "\n",
    "        #     print(B_hessian2)\n",
    "\n",
    "        # print(torch.all(torch.isclose(W_hessian, W_hessian2)))\n",
    "\n",
    " \n",
    "\n",
    "        hessians = torch.zeros_like(y_pred)\n",
    "\n",
    "        hessians[B_ind] = B_hessian\n",
    "\n",
    "        hessians[W_ind] = W_hessian\n",
    "\n",
    " \n",
    "\n",
    "        return hessians\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "    def calc_grad_hess(self, y_true, y_pred):\n",
    "\n",
    "        y_true = torch.tensor(y_true, dtype=torch.float32).cuda()\n",
    "\n",
    "        y_pred = torch.tensor(y_pred, dtype=torch.float32).cuda()\n",
    "\n",
    " \n",
    "\n",
    "        grad = self.grad(y_true, y_pred)\n",
    "\n",
    "        hess = self.hess(y_true, y_pred)\n",
    "\n",
    " \n",
    "\n",
    "        # del y_true\n",
    "\n",
    "        # del y_pred\n",
    "\n",
    " \n",
    "\n",
    "        grad = grad.cpu().detach().numpy()\n",
    "\n",
    "        hess = hess.cpu().detach().numpy()\n",
    "\n",
    " \n",
    "\n",
    "        # return grad, np.ones(grad.shape)\n",
    "\n",
    "        return grad, hess\n",
    "\n",
    " \n",
    "\n",
    "    def rocstar_obj(self, preds, train_data):\n",
    "\n",
    "        y = train_data.get_label()\n",
    "\n",
    "        p = special.expit(preds)\n",
    "\n",
    " \n",
    "\n",
    "        grad, hess = self.calc_grad_hess(y, p)\n",
    "\n",
    " \n",
    "\n",
    "        return grad, hess\n",
    "\n",
    " \n",
    "\n",
    "    def rocstar_eval(self, preds, train_data):\n",
    "\n",
    "        y = train_data.get_label()\n",
    "\n",
    "        p = special.expit(preds)\n",
    "\n",
    " \n",
    "\n",
    "        loss_metric = 'bce_loss' if self.bce_epoch > 0 else 'rocstar_loss'\n",
    "\n",
    "        loss = self.calc_loss(torch.tensor(y, dtype=torch.float32).cuda(), torch.tensor(p, dtype=torch.float32).cuda())\n",
    "\n",
    "        is_higher_better = False\n",
    "\n",
    " \n",
    "\n",
    "        return loss_metric, loss.item(), is_higher_better"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-28T20:24:06.911610200Z",
     "start_time": "2024-11-28T20:24:06.870614100Z"
    }
   },
   "id": "605e78ba3552d00",
   "execution_count": 107
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_model(X_train, y_train, X_fit, X_val, y_fit, y_val):\n",
    "    fit = lgb.Dataset(X_fit, y_fit, free_raw_data=False)\n",
    "    val = lgb.Dataset(X_val, y_val, reference=fit, free_raw_data=False)\n",
    "    gamma = 0.8\n",
    "    \n",
    "    rocstar = ROCStarLoss(gamma=gamma)\n",
    "\n",
    "    params = {\n",
    "        'learning_rate': 0.03,\n",
    "        'num_leaves': 100,\n",
    "        'gamma': 0.62,\n",
    "        'is_unbalance': True,\n",
    "        'objective': rocstar.rocstar_obj\n",
    "    }\n",
    "\n",
    "    print('#######   Training LightGBM with roc-star   #######')\n",
    "    model = lgb.train(\n",
    "        params=params,\n",
    "        train_set=fit,\n",
    "        num_boost_round=10000,\n",
    "        valid_sets=(fit, val),\n",
    "        valid_names=('fit', 'val'),\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=300),\n",
    "            lgb.log_evaluation(period=100)\n",
    "        ],\n",
    "        feval=rocstar.rocstar_eval,\n",
    "    )\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-28T20:24:06.921713100Z",
     "start_time": "2024-11-28T20:24:06.905610800Z"
    }
   },
   "id": "fa29d99f2a51de92",
   "execution_count": 108
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######   Training LightGBM with roc-star   #######\n",
      "[LightGBM] [Warning] Unknown parameter: gamma\n",
      "[LightGBM] [Warning] Unknown parameter: gamma\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013546 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 170883, number of used features: 30\n",
      "[LightGBM] [Warning] Unknown parameter: gamma\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[100]\tfit's rocstar_loss: 1.25394e+07\tval's rocstar_loss: 1.65223e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[200]\tfit's rocstar_loss: 1.25394e+07\tval's rocstar_loss: 1.65223e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[300]\tfit's rocstar_loss: 1.25394e+07\tval's rocstar_loss: 1.65223e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "Early stopping, best iteration is:\n",
      "[24]\tfit's rocstar_loss: 85126.1\tval's rocstar_loss: 486114\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    random_state=42,\n",
    "    test_size=0.2\n",
    ")\n",
    "\n",
    "X_fit, X_val, y_fit, y_val = train_test_split(\n",
    "    X_train, y_train,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rocstar_model = train_model(X_train, y_train, X_fit, X_val, y_fit, y_val)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-28T20:24:38.553833500Z",
     "start_time": "2024-11-28T20:24:06.917714400Z"
    }
   },
   "id": "fb5f515b5b5da519",
   "execution_count": 109
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def predict_results(model, X_test, y_test):\n",
    "    y_pred = special.expit(model.predict(X_test))\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "\n",
    "    optimal_idx = (tpr - fpr).argmax()\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    \n",
    "    y_pred_binary = (y_pred >= optimal_threshold).astype(int)\n",
    "    # y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "    print(f\"Test's ROC AUC: {roc_auc_score(y_test, y_pred):.5f}\")\n",
    "    print(f\"Test's logloss: {log_loss(y_test, y_pred):.5f}\")\n",
    "\n",
    "    return y_pred, y_pred_binary"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-28T20:24:38.575949Z",
     "start_time": "2024-11-28T20:24:38.557833500Z"
    }
   },
   "id": "b401907cb33a3288",
   "execution_count": 110
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test's ROC AUC: 0.98151\n",
      "Test's logloss: 0.05339\n"
     ]
    }
   ],
   "source": [
    "y_pred, y_pred_binary = predict_results(model, X_test, y_test)\n",
    "\n",
    "# binary_predictions = pd.DataFrame({\"y_true\": y_test, \"y_pred\": y_pred_binary})\n",
    "# binary_predictions.to_csv(\"artifacts/predictions/lightgbm_rocstar_predictions.csv\", index=False)\n",
    "\n",
    "predictions = pd.DataFrame({\"y_true\": y_test, \"y_pred\": y_pred})\n",
    "predictions.to_csv(\"artifacts/lightgbm_preds/lightgbm_rocstar_continuous_predictions.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-28T20:24:38.880652100Z",
     "start_time": "2024-11-28T20:24:38.573948400Z"
    }
   },
   "id": "1470ed1b55dcf6e9",
   "execution_count": 111
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-28T20:24:38.897428300Z",
     "start_time": "2024-11-28T20:24:38.882651400Z"
    }
   },
   "id": "1a4f76299086f81c",
   "execution_count": 111
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ec6919e346878140"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
